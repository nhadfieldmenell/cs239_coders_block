(dp0
S'imports'
p1
c__builtin__
set
p2
((lp3
S'tensorflow'
p4
atp5
Rp6
sS'code'
p7
S'import tensorflow as tf\n\n# Parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\ndisplay_step = 1\n\n# Network Parameters\nn_hidden_1 = 256 # 1st layer number of features\nn_hidden_2 = 256 # 2nd layer number of features\nn_input = 784 # MNIST data input (img shape: 28*28)\nn_classes = 10 # MNIST total classes (0-9 digits)\n\n# tf Graph input\nx = tf.placeholder("float", [None, n_input])\ny = tf.placeholder("float", [None, n_classes])\n\n\n# Create model\ndef multilayer_perceptron(x, weights, biases):\n    # Hidden layer with RELU activation\n    layer_1 = tf.add(tf.matmul(x, weights[\'h1\']), biases[\'b1\'])\n    layer_1 = tf.nn.relu(layer_1)\n    # Hidden layer with RELU activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights[\'h2\']), biases[\'b2\'])\n    layer_2 = tf.nn.relu(layer_2)\n    # Output layer with linear activation\n    out_layer = tf.matmul(layer_2, weights[\'out\']) + biases[\'out\']\n    return out_layer\n\n# Store layers weight & bias\nweights = {\n    \'h1\': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n    \'h2\': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n    \'out\': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n}\nbiases = {\n    \'b1\': tf.Variable(tf.random_normal([n_hidden_1])),\n    \'b2\': tf.Variable(tf.random_normal([n_hidden_2])),\n    \'out\': tf.Variable(tf.random_normal([n_classes]))\n}\n\n# Construct model\npred = multilayer_perceptron(x, weights, biases)\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)'
p8
sS'methods'
p9
(lp10
S'tf.placeholder'
p11
aS'tf.placeholder'
p12
aS'multilayer_perceptron'
p13
aS'tf.reduce_mean'
p14
aS'learning_rate.tf.train.AdamOptimizer.minimize'
p15
aS'tf.global_variables_initializer'
p16
aS'tf.Session'
p17
aS'tf.add'
p18
aS'tf.nn.relu'
p19
aS'tf.add'
p20
aS'tf.nn.relu'
p21
aS'tf.Variable'
p22
aS'tf.Variable'
p23
aS'tf.Variable'
p24
aS'tf.Variable'
p25
aS'tf.Variable'
p26
aS'tf.Variable'
p27
aS'tf.nn.softmax_cross_entropy_with_logits'
p28
aS'sess.run'
p29
aS'tf.matmul'
p30
aS'tf.matmul'
p31
aS'tf.matmul'
p32
aS'tf.random_normal'
p33
aS'tf.random_normal'
p34
aS'tf.random_normal'
p35
aS'tf.random_normal'
p36
aS'tf.random_normal'
p37
aS'tf.random_normal'
p38
aS'tf.train.AdamOptimizer'
p39
as.